{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6019472,"sourceType":"datasetVersion","datasetId":3445072}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install pot","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport os\nimport json\nimport random\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom transformers import BertTokenizer, BertModel\nfrom torch.utils.data import DataLoader, Dataset\nimport ot","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def draw_samples(df):\n    fig, axes = plt.subplots(5, 2, figsize=(12, 24))\n    \n    sample_data = df.groupby(\"image_filepath\")[\"caption\"].agg(list).iloc[:5]\n    \n    for i, (index, sample) in enumerate(sample_data.items()):\n        img = Image.open(index)\n        axes[i, 0].imshow(img)\n        axes[i, 0].axis(\"off\")\n        for j, cap in enumerate(sample):\n            axes[i, 1].text(0, 0.9 - 0.2 * j, cap, fontsize=14)\n        axes[i, 1].axis(\"off\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainval_image_dir = '/kaggle/input/coco-image-caption/train2014/train2014'\ntrainval_captions_dir = '/kaggle/input/coco-image-caption/annotations_trainval2014/annotations/captions_train2014.json'\n\ntest_image_dir = '/kaggle/input/coco-image-caption/val2017/val2017'\ntest_captions_dir = '/kaggle/input/coco-image-caption/annotations_trainval2017/annotations/captions_val2017.json'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_filepaths = np.array([os.path.join(trainval_image_dir, f) for f in os.listdir(trainval_image_dir)])\nrand_indices = np.arange(len(all_filepaths))\nnp.random.shuffle(rand_indices)\n\nsplit = int(len(all_filepaths)*0.8)\n\ntrain_filepaths, valid_filepaths = all_filepaths[rand_indices[:split]], all_filepaths[rand_indices[split:]] \n\nprint(f\"Train dataset size: {len(train_filepaths)}\")\nprint(f\"Valid dataset size: {len(valid_filepaths)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(trainval_captions_dir, 'r') as f:\n    trainval_data = json.load(f)\n    \ntrainval_captions_df = pd.json_normalize(trainval_data, \"annotations\")\ntrainval_captions_df[\"image_filepath\"] = trainval_captions_df[\"image_id\"].apply(\n    lambda x: os.path.join(trainval_image_dir, 'COCO_train2014_' + format(x, '012d') + '.jpg')\n)\n\ndef preprocess_captions(df):\n    df[\"preprocessed_caption\"] = df[\"caption\"].str.lower().str.replace('[^\\w\\s]', '', regex=True)\n    return df\n\ntrain_captions_df = trainval_captions_df[trainval_captions_df[\"image_filepath\"].isin(train_filepaths)]\ntrain_captions_df = preprocess_captions(train_captions_df)\n                                       \nvalid_captions_df = trainval_captions_df[trainval_captions_df[\"image_filepath\"].isin(valid_filepaths)]\nvalid_captions_df = preprocess_captions(valid_captions_df)\n\nwith open(test_captions_dir, 'r') as f:\n    test_data = json.load(f)\n    \ntest_captions_df = pd.json_normalize(test_data, \"annotations\")\ntest_captions_df[\"image_filepath\"] = test_captions_df[\"image_id\"].apply(\n    lambda x: os.path.join(test_image_dir, format(x, '012d') + '.jpg')\n)\ntest_captions_df = preprocess_captions(test_captions_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_captions_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"draw_samples(train_captions_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resnet = torchvision.models.resnet50(pretrained=True)\nresnet = nn.Sequential(*list(resnet.children())[:-1])\nresnet.train()\n\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nbert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ImageCaptioningDataset(Dataset):\n    def __init__(self, df, tokenizer, transform):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_path = self.df.iloc[idx]['image_filepath']\n        image = Image.open(img_path).convert('RGB')\n        image = self.transform(image)\n        \n        caption = self.df.iloc[idx]['preprocessed_caption']\n        inputs = self.tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n        \n        return image, inputs.input_ids.squeeze(), inputs.attention_mask.squeeze()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CaptioningModel(nn.Module):\n    def __init__(self, resnet, bert_model):\n        super(CaptioningModel, self).__init__()\n        self.resnet = resnet\n        self.bert = bert_model\n        self.fc = nn.Linear(2048, 768)\n\n    def forward(self, image, input_ids, attention_mask):\n        image_features = self.resnet(image).view(image.size(0), -1)\n        image_features = self.fc(image_features)\n        \n        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        text_embeddings = bert_outputs.last_hidden_state.mean(dim=1)\n        \n        return image_features, text_embeddings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def optimal_transport_loss(image_features, text_embeddings):\n    image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n    text_embeddings = text_embeddings / text_embeddings.norm(p=2, dim=-1, keepdim=True)\n    \n    cost_matrix = torch.cdist(image_features, text_embeddings, p=2).detach().cpu().numpy()\n\n    a = np.ones(image_features.size(0)) / image_features.size(0)\n    b = np.ones(text_embeddings.size(0)) / text_embeddings.size(0)\n    \n    ot_distance = ot.emd2(a, b, cost_matrix)\n    \n    return torch.tensor(ot_distance, device=image_features.device, requires_grad=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"subset = train_captions_df[:8000]\n# subset = train_captions_df\ndataset = ImageCaptioningDataset(subset, tokenizer, transform)\ndataloader = DataLoader(dataset, batch_size=128, shuffle=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ncaptioning_model = CaptioningModel(resnet, bert_model).to(device)\noptimizer = torch.optim.Adam(captioning_model.parameters(), lr=0.0001)\n\nnum_epochs = 20\nfor epoch in range(num_epochs):\n    for images, input_ids, attention_mask in dataloader:\n        images = images.to(device)\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        \n        optimizer.zero_grad()\n        \n        image_features, text_embeddings = captioning_model(images, input_ids, attention_mask)\n        \n        loss = optimal_transport_loss(image_features, text_embeddings)\n        \n        loss.backward()\n        optimizer.step()\n        \n    print(f\"Epoch {epoch + 1}/{num_epochs}, OT Loss: {loss.item()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"captioning_model.eval()\n\nimg_file_path = subset.iloc[0]['image_filepath']\nimg_caption = subset.iloc[0]['preprocessed_caption']\n\nimage_ori = Image.open(img_file_path).convert('RGB')\nimage = transform(image_ori)\nimage = image.to(device).unsqueeze(0)\n\ntext = img_caption\n# text = \"a woman is playing badminton\"\ntokenized = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\ninput_ids = tokenized[\"input_ids\"].to(device)\nattention_mask = tokenized[\"attention_mask\"].to(device)\n\nwith torch.no_grad():\n    image_features, text_embeddings = captioning_model(image, input_ids, attention_mask)\n\n# print(\"Image Features:\", image_features)\n# print(\"Text Embeddings:\", text_embeddings)\nplt.imshow(image_ori)\nplt.axis('off')\nplt.show()\nprint(\"Caption:\", text)\nprint(\"Loss:\", optimal_transport_loss(image_features, text_embeddings).item())","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}